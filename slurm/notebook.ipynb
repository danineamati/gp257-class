{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2825d6cb-e15e-4a79-a3c7-c1a2b5954beb",
   "metadata": {},
   "source": [
    "# Getting started with slurm\n",
    "\n",
    "\n",
    "## Summary \n",
    "\n",
    "In this lab you will learn some of the basics of slurm and a few of the more advanced features that you might helpful in the future. We will be doing this lab on our class cluster but everything we are doing will also work on Stanford's Sherlock system (or any other slurm cluster) often with more interesting results because they are bigger more complex systems.\n",
    "\n",
    "\n",
    "# Slurm basics\n",
    "\n",
    "\n",
    "Slurm is a workload manager. Its jobs is to efficiently and fairly provide cluster resources. A slurm controlled cluster is ususally broken up into parittions where different sets of rules on use and are often composed of different hardware.  You use slurm resources by making a request to the slurm controller node for resources.  Your request is evaluated for priority based on what are referred to as \"fairshare\" rules. When there are free resources and you have the top priority you are granted access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178589bb-4449-4a10-843f-264227c0bc10",
   "metadata": {},
   "source": [
    "## sinfo\n",
    "\n",
    "The sinfo command is very valuable way to find out about a cluster. Using the command without any options will give you a general idea about the cluster you are using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96704bf6-93cd-4719-a98a-53d411706bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
      "debug*       up   infinite      8  idle~ slurm-gp257-devel-compute-0-[4-11]\n",
      "debug*       up   infinite      1    mix slurm-gp257-devel-compute-0-2\n",
      "debug*       up   infinite      3  alloc slurm-gp257-devel-compute-0-[0-1,3]\n",
      "cpu          up   infinite      5  idle~ slurm-gp257-devel-compute-1-[0-4]\n",
      "t4           up   infinite     10  idle~ slurm-gp257-devel-compute-2-[0-9]\n",
      "preempt      up   infinite     16 alloc# slurm-gp257-devel-compute-3-[2-17]\n",
      "preempt      up   infinite     32  idle~ slurm-gp257-devel-compute-3-[18-49]\n",
      "preempt      up   infinite      2  alloc slurm-gp257-devel-compute-3-[0-1]\n"
     ]
    }
   ],
   "source": [
    "!sinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fc7315-2f53-46de-b453-5fc83b279177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "              9624     debug     bash jdstitt_  R    2:26:05      1 slurm-gp257-devel-compute-0-0\n",
      "             10627     debug     bash ellopes_  R      23:59      1 slurm-gp257-devel-compute-0-1\n",
      "             10631     debug     bash ppettit_  R       9:47      1 slurm-gp257-devel-compute-0-2\n",
      "             10636     debug     bash dneamati  R       4:23      1 slurm-gp257-devel-compute-0-3\n",
      "           10637_1   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-2\n",
      "           10637_2   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-3\n",
      "           10637_3   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-4\n",
      "           10637_4   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-5\n",
      "           10637_5   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-6\n",
      "           10637_6   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-7\n",
      "           10637_7   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-8\n",
      "           10637_8   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-9\n",
      "           10637_9   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-10\n",
      "          10637_10   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-11\n",
      "          10637_11   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-12\n",
      "          10637_12   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-13\n",
      "          10637_13   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-14\n",
      "          10637_14   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-15\n",
      "          10637_15   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-16\n",
      "          10637_16   preempt test_job tculliso CF       1:05      1 slurm-gp257-devel-compute-3-17\n",
      "              9623   preempt     bash jdstitt_  R    2:26:00      1 slurm-gp257-devel-compute-3-0\n",
      "             10630   preempt     bash tculliso  R       9:26      1 slurm-gp257-devel-compute-3-1\n"
     ]
    }
   ],
   "source": [
    "!squeue\n",
    "# I'm in the queue! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58793c-5747-4e40-80c0-0bda6c0e501a",
   "metadata": {},
   "source": [
    "By default the sinfo command gives you the names of the various partitions, the number of nodes in each partition, their status, and the name of the nodes in the various partitions.  This represents a very small amount of the sinfo commands capabilites. For example I can find the number of cores on a given node using the -n (for a specific node) and -O (for specific information) flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18390ac-fa4e-4a15-af63-18817cfe6f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORES               \n",
      "4                   \n"
     ]
    }
   ],
   "source": [
    "!sinfo -n slurm-gp257-devel-compute-0-0 -O CORES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a2f5a-4fa7-49d0-af8d-f8dcb2a04a07",
   "metadata": {},
   "source": [
    "Below are some additional useful options.  Generic resources, Gres, refers to GPUs among other things.\n",
    "\n",
    "\n",
    "| Option | Description|\n",
    "| :--- | ------- |\n",
    "| Cores | Number of cores per socket.|\n",
    "| Disk  | Size of temporary disk space per node in megabytes.|\n",
    "| FreeMem | The total memory, in MB, currently free on the node as reported by the OS. This value is for informational use only and is not used for scheduling.|\n",
    "| Gres  | Generic resources (gres) associated with the nodes. |\n",
    "| GresUsed | Generic resources (gres) currently in use on the nodes. |\n",
    "| Memory | Size of memory per node in megabytes. |\n",
    "| PreemptMode  | Preemption mode. |\n",
    "|  Threads | Number of threads per core. |\n",
    "| Time  |Maximum time for any job in the format \"days-hours:minutes:seconds\". |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec565402-7960-45e7-9957-305f4af2e18e",
   "metadata": {},
   "source": [
    "In the cell below build a table describing the nodes on the cluster using the above commmand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f63198b-fea6-4607-a614-bd26081f2db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION           CORES               TMP_DISK            FREE_MEM            GRES                GRES_USED           \n",
      "debug*              4                   0                   23462-30758         (null)              gpu:0               \n",
      "cpu                 30                  0                   237889-N/A          (null)              gpu:0               \n",
      "t4                  8                   0                   52016-N/A           gpu:1               gpu:0               \n",
      "preempt             30                  0                   237499-237898       (null)              gpu:0               \n"
     ]
    }
   ],
   "source": [
    "!sinfo -O Partition,Cores,Disk,FreeMem,Gres,GresUsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a70242-7820-4d93-a729-b8ba5dca0e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION           NODES               PREEMPT_MODE        THREADS             TIMELIMIT           \n",
      "debug*              12                  OFF                 1                   infinite            \n",
      "cpu                 5                   OFF                 1                   infinite            \n",
      "t4                  10                  OFF                 1                   infinite            \n",
      "preempt             50                  OFF                 1                   infinite            \n"
     ]
    }
   ],
   "source": [
    "!sinfo -O Partition,Nodes,PreemptMode,Threads,Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f2b91-9a5c-4856-a62e-435d08e2f173",
   "metadata": {},
   "source": [
    "The results of describing the nodes in the cluster are below. Note that the `FREE_MEM` entry changes on different runs of the code above.\n",
    "\n",
    "| Partition | `debug` | `cpu` | `t4` | `preempt` |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| Number of Nodes | 12 | 5 | 10 | 50 |\n",
    "| Number of Cores per Node (`Cores`) | 4 | 30 | 8 | 30 |\n",
    "| Temp Disk Size per Node (`Disk`) | 0 | 0 | 0 | 0 |\n",
    "| Total Free Memory (`FreeMem`) in MB | 23462-30758 | 237889-N/A | 52016-N/A | 237499-237898 |\n",
    "| Generic Resources (`Gres`) | (null) | (null) | gpu:1 | (null) |\n",
    "| Generic Resources (`GresUsed`) | gpu:0 | gpu:0 | gpu:0 | gpu:0 |\n",
    "| Preemption Mode (`PreemptMode`) | OFF | OFF | OFF | OFF |\n",
    "| Number of Threads per Core (`Threads`) | 1 | 1 | 1 | 1 |\n",
    "| Maximum Time for a Job (`Time`) | infinite | infinite | infinite | infinite |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263736a-e55e-4845-b852-b6a6678ef5f6",
   "metadata": {},
   "source": [
    "# squeue\n",
    "\n",
    "Thre squeue command gives us information about what is actually running on the cluster.  Often it useful to use the -p partiton option to reduce the infomation it produces on a cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4706e9-bccb-412a-81ff-c5a08f71ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "              9624     debug     bash jdstitt_  R    2:30:37      1 slurm-gp257-devel-compute-0-0\n",
      "             10627     debug     bash ellopes_  R      28:31      1 slurm-gp257-devel-compute-0-1\n",
      "             10631     debug     bash ppettit_  R      14:19      1 slurm-gp257-devel-compute-0-2\n",
      "             10636     debug     bash dneamati  R       8:55      1 slurm-gp257-devel-compute-0-3\n",
      "             10654     debug results1 ellopes_  R       0:01      1 slurm-gp257-devel-compute-0-2\n"
     ]
    }
   ],
   "source": [
    "! squeue -p debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f15a5e-99a0-4626-9177-890036de46c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "              9624     debug     bash jdstitt_  R    2:30:45      1 slurm-gp257-devel-compute-0-0\n",
      "             10627     debug     bash ellopes_  R      28:39      1 slurm-gp257-devel-compute-0-1\n",
      "             10631     debug     bash ppettit_  R      14:27      1 slurm-gp257-devel-compute-0-2\n",
      "             10636     debug     bash dneamati  R       9:03      1 slurm-gp257-devel-compute-0-3\n",
      "             10655     debug results2 ellopes_  R       0:02      1 slurm-gp257-devel-compute-0-2\n",
      "              9623   preempt     bash jdstitt_  R    2:30:40      1 slurm-gp257-devel-compute-3-0\n",
      "             10630   preempt     bash tculliso  R      14:06      1 slurm-gp257-devel-compute-3-1\n"
     ]
    }
   ],
   "source": [
    "# Compare to squeue alone\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "973401d2-8425-4923-9f13-5c3fdd5ce318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeue: error: Invalid user: dneamati\n",
      "\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "!squeue --user=dneamati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00840058-02c2-4abd-87c8-f232e091b7d8",
   "metadata": {},
   "source": [
    "The results of the command gives you all the jobs that have been submitted to the cluster. It displays the jobid, the parition the job has been submitted, its status, how long it has been running, and the nodes the job is running on.  Below is the list of status possibilites.\n",
    "\n",
    "\n",
    "|Status|Code|Explaination|\n",
    "| ----- | ------ | ----- |\n",
    "|COMPLETED|CD|The job has completed successfully.|\n",
    "|COMPLETING|CG|The job is finishing but some processes are still active.|\n",
    "|FAILED|F|The job terminated with a non-zero exit code and failed to execute.|\n",
    "|PENDING|PD|The job is waiting for resource allocation. It will eventually run.|\n",
    "|PREEMPTED|PR|The job was terminated because of preemption by another job.|\n",
    "|RUNNING|R|The job currently is allocated to a node and is running.|\n",
    "|SUSPENDED|S| A running job has been stopped with its cores released to other jobs.|\n",
    "|STOPPED|ST|A running job has been stopped with its cores retained.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5cd69-8081-46f0-aada-b7c2ba272059",
   "metadata": {},
   "source": [
    "The squeue and sinfo together provide a powerful tool in combination if you can't figure out why your job isn't starting (or fails)\". Some complex examples include\n",
    "- If you are requesting a job using a number of nodes you can see how many nodes are being currently used. Is it all by one user?\n",
    "    - With `squeue`, we can see the users on each node (and the number of nodes). \n",
    "- If your job will not start, are you requesting more memory that in on the queue, more processors?\n",
    "    - With `sinfo`, specifically the '-o' tag can be used list the memory and processor specifications for a node on a partition.\n",
    "- Are other users using up all of the memory /GPUs currently?\n",
    "    - With `sinfo`, we can see which nodes are allocated, mixed, or free. This can be especially useful within a partition. Specifically with the '-o' tag and `GRES` and `GRES_USED`, we can further check the GPU usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a1fa7-3ab1-40c3-b3cc-da3ab601e895",
   "metadata": {},
   "source": [
    "# salloc, sbatch, and srun\n",
    "\n",
    "There are two general ways to request slurm resources, salloc and sbatch.  The easiest way to remember the differece is to use salloc when you want to do something interactivelty, sbatch when you are wanting a submit a job that runs eventually.  srun is used to launch jobs on these allocated resources but can be used directly (in whcih case it calls an salloc behind the scenes). \n",
    "\n",
    "The command you used to start this lab is a simple example\n",
    "\n",
    "srun --pty --x11 /bin/bash\n",
    "\n",
    "Here we requesteed a node on the default partition, with the default amount of memory, for the default amount of time, with a single core, said we wanted to run in terminal mode, and ran the command bash. \n",
    "\n",
    "In almost all cases you want to specify the resources you want to use it and the time you want to use them. The primary reason you want to specify the resources you need is the defults are often not approriate. You many need more memory, need more cores, etc. Almost as important is that it can often time give you an advatnage with the scheduler. If your job doesn't take much memory when another user is using most of the memory on a node, your job will start, while someone using the default might not. If the scheduler is trying to run a job that requires many cores, specifying your job will only run for a few minutes, the scheduler will run your job while waiting for other jobs to finish.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14befb10-e4b9-4204-ad74-ed5a3dbff73b",
   "metadata": {},
   "source": [
    "# Specifying job parameters (the basics)\n",
    "\n",
    "We can specify slurm configuration parameters on the command line, or when using sbatch in the submission shell file by begining a line with #SBATCH before specify an option.\n",
    "\n",
    "We can specify the parition using the -partition partName flag. The length of a job with --time=day-hours:minutes:seconds.  We can specify to use gpus by using the gres flag  --gres=gpu[[:type]:number].\n",
    "\n",
    "\n",
    "## Specifying tasks\n",
    "\n",
    "Slurm thinks of a job that has one or tasks. These task(s) run on one or more nodes.  We specify the number of tasks using --ntasks=ntasks.  If each parallel tasks uses multiple cores we can use --cpus-per-task=ncpus.  We can specify multiple tasks per core (--tasks-per-cpu=tasks) or node (--tasks-per-nodes). \n",
    "\n",
    "## Memory\n",
    "\n",
    "There are a number of ways to specify. We can specify the total memory for a job with --mem=<size>units. We can also specify memory per cpu --mem-per-cpu=<size>[units] and per GPU --mem-per-gpu=<size>[units].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21797228",
   "metadata": {},
   "source": [
    "# Simple submission\n",
    "\n",
    "In this part of the lab you will write a simple program to calculate pi using Leibniz’s formula.\n",
    "\n",
    "\n",
    "X = 4 - 4/3 + 4/5 - 4/7 + 4/9 - ....\n",
    "\n",
    "This series is never-ending, the more the terms this series contains, the closer the value of X converges to Pi value.\n",
    "\n",
    "Use the next three cells to \n",
    "\n",
    "- write and save a python script that caclulates Pi using Leibniz formula. You should sum to 1000000 terms.\n",
    "- write a sbatch that\n",
    "    - submits a job to the compute partition\n",
    "    - submits a time limit of 2 hours for the job\n",
    "    - specifies one GB of memory\n",
    "    - runs on a single core\n",
    "- submits the jobs to the cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4077987",
   "metadata": {},
   "source": [
    "%%writefile leib.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Do stuff\n",
    "\n",
    "f=open(\"result1.txt\",\"w\")\n",
    "f.write(res)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84870865",
   "metadata": {},
   "source": [
    "%%writefile submit.sh\n",
    "#SBATCH --ntasks=1\n",
    "\n",
    "\n",
    "spack load python\n",
    "python3 ./leib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df0f4f",
   "metadata": {},
   "source": [
    "!sbatch submit.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f3ab1",
   "metadata": {},
   "source": [
    "# Slurm advanced\n",
    "\n",
    "Slurm also has a number of advanced options that can be quite useful.  \n",
    "\n",
    "## Resubmitting\n",
    "\n",
    "Clusters are always trying to maximize there usage.  As a result they often have a mechanism that allows unused resources used for free at or a reduced cost. In general you get to use these resources until someone who has/or is willing to pay/has paid a higher cost. At Stanford, the Sherlock clusters owner's queue operates in this manner. If you submit a job to the queue it will use all free resources if the owner of the resource requests those resources your job will be killed. GCP has the concept of preemitbible nodes, which opperate at a much lower cost, but the jobs can be killed at any time.\n",
    "\n",
    "Many applications can take advantage of these potentially killed resources.  They are ideal for jobs that take a short time and don't use many cores. Slurm provides the option --requeue to enable the use of these resources.  Any job that doesn't complete is automatically resubmitted to the queue.\n",
    "\n",
    "## Job arrays\n",
    "\n",
    "There are many cases where you want to run a series of very similar jobs. Each job could be submitted in sequence but this can overload a slurm controller load.  Slurm job arrays provide an effective way to approach these jobs. The command line option --array offers a potential solution. You can submit an array in many forms:\n",
    "\n",
    "- --array=0-31    # jobs 0 1 2 3 4 5 .. 31\n",
    "- --array=1,3,5,7 # jobs 1 3 5 7\n",
    "-  --array=1-7:2  # jobs 1 3 4 5 7\n",
    "\n",
    "Each job will set the environmental variable SLURM_ARRAY_TASK_ID with the corresponding ID.\n",
    "\n",
    "\n",
    "## Dependency\n",
    "\n",
    "Another powerful feature of slurm is the ability to build dependencies. Basically guarantee that a task doesn't start until a given condition has been met. You can set a dependency using --dependency=<type:job_id[:job_id] where type is one of the following.\n",
    "\n",
    "\n",
    "- after:jobid[:jobid...]\tjob can begin after the specified jobs have started\n",
    "- afterany:jobid[:jobid...]\tjob can begin after the specified jobs have terminated\n",
    "- afternotok:jobid[:jobid...]\tjob can begin after the specified jobs have failed\n",
    "- afterok:jobid[:jobid...]\tjob can begin after the specified jobs have run to completion with an exit code of zero (see the user guide for caveats).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bc972",
   "metadata": {},
   "source": [
    "# Complex submission\n",
    "\n",
    "A second way to calculate pi is to use random numbers. You can calculate pi by taking by \n",
    " - choosing two random numbers between 0 and 1\n",
    " - checking whether to sum of the squares of those numbers is <= 1\n",
    " - 4 times the fraction of numbers that meet this criteria will be equal to pi\n",
    "\n",
    "\n",
    "Your job is to\n",
    "\n",
    "- write a program that follows the above procedure to estimate pi using a large number of tests\n",
    "    - it should seed to random number generated based on reading the environmental variable SLURM_ARRAY_TASK_ID.\n",
    "    - it should write the estimate to a file that uses the SLURM_ARRAY_TASK_ID in its name\n",
    "- You should write a second program that reads a series of files with the name pattern above and averaes them and writes the result to result2.txt.\n",
    "- You should write two slurm submission scripts\n",
    "    - The First should submit to the preempt partition a job array of 1000 and resubmit if the job fails (make sure you test your code by running it on this node before submitting)\n",
    "    - The second job should depend on the first job finishing with an exit code 0 befofre running the second python script. This job should run on debug partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034b739",
   "metadata": {},
   "source": [
    "# Finishing up\n",
    "\n",
    "Add this lab to your class github site after adding all of the files that you have created for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123652ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
